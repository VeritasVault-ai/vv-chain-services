{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fe158d64322d32",
   "metadata": {},
   "source": [
    "# Model Specification\n",
    "We will use the Black-Litterman (BL) approach for calculating weights of assets in our portfolio. We use the classical approach with some adjustments, that will be discussed in what is to follow.\n",
    "\n",
    "Under BL, the expected returns of the assets in our portfolio are given by\n",
    "$$\n",
    "\\mu_{BL} = \\left[ (\\tau \\Sigma)^{-1} + P^\\top \\Omega^{-1} P \\right]^{-1} \\left[ (\\tau \\Sigma)^{-1} \\Pi + P^\\top \\Omega^{-1} Q \\right],\n",
    "$$\n",
    "where $\\mu_{BL}$ is the expected return of the portfolio, $\\tau$ is the uncertainty in the prior, $\\Sigma$ is the covariance matrix of asset returns, $P$ is a picking matrix, with row vectors that represent the weights used in assets for expressing relative / absolute views, $\\Omega$ is the diagonal covariance matrix of confidences of our expected returns $Q$ (view specified by investor), and $\\Pi$ is the implied excess return of our assets.\n",
    "\n",
    "Now,\n",
    "$$\n",
    "\\Pi = \\lambda \\Sigma w_{mkt},\n",
    "$$\n",
    "where $\\lambda$ is the investor risk aversion co-efficient, and $w_{mkt}$ are the market capitalization weights of our assets, determined by TVL in our case (*_todo: expand on this_*).\n",
    "\n",
    "Before we jump into modelling, let's first explore our data. We extract the largest 40 assets by TVL."
   ]
  },
  {
   "cell_type": "code",
   "id": "81a9587c6999485b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:45:34.798313Z",
     "start_time": "2025-05-26T09:45:34.776507Z"
    }
   },
   "source": [
    "# Exploring the universe of data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from main_app.infrastructure.defi_llama import get_pool_summary_data\n",
    "\n",
    "# Variable to specify whether to load data from a file\n",
    "load_from_file = True\n",
    "save_to_file = False\n",
    "file_name = \"./data/all_assets_df.csv\"\n",
    "\n",
    "if load_from_file:\n",
    "    # Load data from file\n",
    "    df = pd.read_csv(file_name)\n",
    "else:\n",
    "    # Retrieve data\n",
    "    pool_map = get_pool_summary_data()\n",
    "    df = pd.DataFrame()\n",
    "    for symbol, pools in pool_map.items():\n",
    "        tvl = 0\n",
    "        weighted_apy = 0\n",
    "        num_pools = len(pools)\n",
    "        largest_pool_id = \"\"\n",
    "        largest_pool_tvl = 0\n",
    "        for pool in pools:\n",
    "            tvl += pool.tvlUsd\n",
    "            weighted_apy += pool.tvlUsd * pool.apy / 100\n",
    "            if pool.tvlUsd > largest_pool_tvl:\n",
    "                largest_pool_tvl = pool.tvlUsd\n",
    "                largest_pool_id = pool.pool\n",
    "\n",
    "        weighted_apy = weighted_apy / tvl\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame({'symbol': [symbol], 'tvlUsd': [tvl], 'apy': [weighted_apy], 'pools': [num_pools], 'largest_pool_id': [largest_pool_id], 'largest_pool_tvl': [largest_pool_tvl], 'largest_pool_pct_of_tvl': [largest_pool_tvl / tvl * 100]})])\n",
    "\n",
    "    # Sort\n",
    "    df = df.sort_values(by='tvlUsd', ascending=False)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    if save_to_file:\n",
    "        df.to_csv(file_name, index=False)\n",
    "\n",
    "# Retrieve largest 40 assets by TVL\n",
    "largest_assets_df = df.head(40)\n",
    "\n",
    "# Print the first 5 rows of the largest assets DataFrame\n",
    "print(largest_assets_df.head(5))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   symbol       tvlUsd       apy  pools                       largest_pool_id  \\\n",
      "0   STETH  23486375374  0.026812     18  747c1d2a-c668-4682-b9f9-296708a3dd90   \n",
      "1   WEETH  11697415279  0.021623     55  46bd2bdf-6d92-4066-b482-e885ee172264   \n",
      "2    WBTC   7084373652  0.000574    132  7e382157-b1bc-406d-b17b-facba43b716e   \n",
      "3  WSTETH   6945164662  0.000693    121  e6435aae-cbe9-4d26-ab2c-a4d533db9972   \n",
      "4   WBETH   6038513881  0.025876      7  80b8bf92-b953-4c20-98ea-c9653ef2bb98   \n",
      "\n",
      "   largest_pool_tvl  largest_pool_pct_of_tvl  \n",
      "0       23343461123                99.391501  \n",
      "1        6250222218                53.432507  \n",
      "2        4182358678                59.036393  \n",
      "3        2966228157                42.709256  \n",
      "4        5492332610                90.955038  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "9f2757c84a56840f",
   "metadata": {},
   "source": [
    "Importantly, note the _largest_pool_pct_of_tvl_, being the percentage of the total TVL (across all pools) that consists of the largest pool. We note that the largest pool is always significant enough to only look at the data from the largest pool as representative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa872522be6ef0c7",
   "metadata": {},
   "source": [
    "# Parameter estimation\n",
    "\n",
    "## Determining the risk aversion co-efficient\n",
    "We wish to determine the risk aversion coefficient, $\\lambda$, that represents the amount of risk an investor is willing to take. Higher $\\lambda$ means less risky, and conversely, lower $\\lambda$ means higher risk tolerance.\n",
    "\n",
    "In the Black-Litterman model used for Equities this is often set to 2.5, based on long-run estimates of equity risk premiums and volatility. Works as a good default for institutional settings.\n",
    "\n",
    "We wish to adapt this for the defi market by considering a DeFi benchmark portfolio, to be used as an index $I$, using the same method by using:\n",
    "$$\n",
    "\\lambda = \\frac{\\mathbb{E}[R_I]-r_f}{\\sigma_P^2},\n",
    "$$\n",
    "where $R_I$ are the index (portfolio) returns, $r_f$ is the risk free rate, and $\\sigma$ is the standard deviation of the index returns.\n",
    "\n",
    "First we load the data for the largest assets that will constitute the index."
   ]
  },
  {
   "cell_type": "code",
   "id": "bda92795f102deda",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-26T10:52:55.325569Z",
     "start_time": "2025-05-26T10:52:55.309768Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from main_app.infrastructure.market_data import get_historical_data_for_symbol, load_symbol_to_address_mapping\n",
    "\n",
    "symbols = largest_assets_df['symbol'].to_list()\n",
    "largest_pools = largest_assets_df['largest_pool_id'].to_list()\n",
    "\n",
    "# Get current path and join with static data path\n",
    "current_path = os.getcwd()\n",
    "mapping_file_path = os.path.join(current_path, \"..\\\\static_data\\\\symbol_to_contract_address_map.json\")\n",
    "symbol_to_address_mapping = load_symbol_to_address_mapping(mapping_file_path)\n",
    "\n",
    "# Variables to specify file operations\n",
    "load_from_file = True\n",
    "save_to_file = False\n",
    "df_tvl_file = \"./data/df_largest_assets_tvl.csv\"\n",
    "df_apy_file = \"./data/df_largest_assets_apy.csv\"\n",
    "df_price_file = \"./data/df_largest_assets_price.csv\"\n",
    "\n",
    "if load_from_file:\n",
    "    # Load DataFrames from files\n",
    "    df_tvl = pd.read_csv(df_tvl_file, index_col=0, parse_dates=True)\n",
    "    df_apy = pd.read_csv(df_apy_file, index_col=0, parse_dates=True)\n",
    "    df_price = pd.read_csv(df_price_file, index_col=0, parse_dates=True)\n",
    "else:\n",
    "    df_tvl = None\n",
    "    df_apy = None\n",
    "    df_price = None\n",
    "\n",
    "    # Generate the dataframes\n",
    "    for symbol, pool in zip(symbols, largest_pools):\n",
    "        try:\n",
    "            historic_data_df = get_historical_data_for_symbol(symbol, symbol_to_address_mapping, [pool])\n",
    "        except ValueError as e:\n",
    "            print(f\"Value error, {e}, skipping symbol {symbol}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Exception, {e}, skipping symbol {symbol}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"{symbol} has {len(historic_data_df)} entries.\")\n",
    "\n",
    "        if 'date' in historic_data_df.columns:\n",
    "            if df_tvl is None:\n",
    "                # Set the DataFrame with the first symbol's data\n",
    "                df_tvl = historic_data_df[['date', 'tvlUsd']].rename(columns={'tvlUsd': symbol}).set_index('date')\n",
    "            else:\n",
    "                # Merge without suffix, using the symbol as the column name\n",
    "                df_tvl = pd.merge(df_tvl, historic_data_df[['date', 'tvlUsd']].rename(columns={'tvlUsd': symbol}).set_index('date'),\n",
    "                                  on='date', how='outer')\n",
    "\n",
    "            if df_apy is None:\n",
    "                df_apy = historic_data_df[['date', 'apy']].rename(columns={'apy': symbol}).set_index('date')\n",
    "            else:\n",
    "                df_apy = pd.merge(df_apy, historic_data_df[['date', 'apy']].rename(columns={'apy': symbol}).set_index('date'),\n",
    "                                  on='date', how='outer')\n",
    "\n",
    "            if df_price is None:\n",
    "                df_price = historic_data_df[['date', 'price']].rename(columns={'price': symbol}).set_index('date')\n",
    "            else:\n",
    "                df_price = pd.merge(df_price, historic_data_df[['date', 'price']].rename(columns={'price': symbol}).set_index('date'),\n",
    "                                    on='date', how='outer')\n",
    "\n",
    "    # Save DataFrames to files, if specified\n",
    "    if save_to_file:\n",
    "        if df_tvl is not None:\n",
    "            df_tvl.to_csv(df_tvl_file)\n",
    "        if df_apy is not None:\n",
    "            df_apy.to_csv(df_apy_file)\n",
    "        if df_price is not None:\n",
    "            df_price.to_csv(df_price_file)"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "id": "3a068d8f31c54a8c",
   "metadata": {},
   "source": "Next we filter data  for the index to consist of all assets that have data for the last 365 days"
  },
  {
   "cell_type": "code",
   "id": "ea337c3a4dac02f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T10:53:03.459850Z",
     "start_time": "2025-05-26T10:53:03.446454Z"
    }
   },
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Check if any of the dataframes are None at the beginning of the cell\n",
    "if any(df is None for df in [df_tvl, df_apy, df_price]):\n",
    "    print(\"Error: One or more required DataFrames (df_tvl, df_apy, df_price) are None. Cannot proceed with calculations.\")\n",
    "    exit()\n",
    "\n",
    "# Set the cutoff date to a year prior to today\n",
    "cutoff_date = datetime.today() - timedelta(days=365*2)\n",
    "\n",
    "# Ensure the indexes of all DataFrames are converted to datetime\n",
    "df_tvl.index = pd.to_datetime(df_tvl.index)\n",
    "df_apy.index = pd.to_datetime(df_apy.index)\n",
    "df_price.index = pd.to_datetime(df_price.index)\n",
    "\n",
    "# Filter rows in df_tvl, df_apy, and df_price based on the cutoff date\n",
    "df_tvl = df_tvl[df_tvl.index >= cutoff_date]\n",
    "df_apy = df_apy[df_apy.index >= cutoff_date]\n",
    "df_price = df_price[df_price.index >= cutoff_date]\n",
    "\n",
    "# Remove columns with NaN values from df_tvl, df_apy, and df_price\n",
    "for df in [df_tvl, df_apy, df_price]:\n",
    "    # for each column (being the coin's data), if the first value is nan then remove it from the columns\n",
    "    for column in df.columns:\n",
    "        if pd.isna(df[column].iloc[0]):\n",
    "            df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "    # now filter to ensure that all data is non nan\n",
    "    df.dropna(inplace=True)"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we calculate $\\lambda$",
   "id": "3ba4c6a12048fc15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T11:07:05.925790Z",
     "start_time": "2025-05-26T11:07:05.919677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate daily log returns, including the growth of our assets using the apy\n",
    "df_price_log_returns = np.log(df_price / df_price.shift(1) * (1 + df_apy.shift(1) / 365.0)).dropna().to_numpy()\n",
    "\n",
    "# Calculate weight_market and target_return\n",
    "mean_market_weight_vector = df_tvl.mean().to_numpy()\n",
    "\n",
    "# Create index and calculate prices and returns\n",
    "index_0 = 100\n",
    "index_weights = mean_market_weight_vector / sum(mean_market_weight_vector)\n",
    "index_daily_returns = (index_weights.transpose() @ df_price_log_returns.transpose())\n",
    "index_prices = index_0 * np.cumprod(np.exp(index_daily_returns))\n",
    "index_prices = [index_0] + index_prices.tolist()\n",
    "\n",
    "# Calculate index mean return and standard deviation\n",
    "index_mean_return = np.mean(index_daily_returns)\n",
    "index_std_dev = np.std(index_daily_returns)\n",
    "\n",
    "# Specify risk-free rate\n",
    "risk_free_rate = 0.02 / 365\n",
    "\n",
    "# Calculate lambda\n",
    "l = (index_mean_return - risk_free_rate) / index_std_dev ** 2\n",
    "\n",
    "print(f\"Lambda = {l}\")"
   ],
   "id": "ab84611ffeeac22b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 0.5743247315927997\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b0c69ed79a09380b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
