{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Specification\n",
    "We will use the Black-Litterman (BL) approach for calculating weights of assets in our portfolio. We use the classical approach with some adjustments, that will be discussed in what is to follow.\n",
    "\n",
    "Under BL, the expected returns of the assets in our portfolio are given by\n",
    "$$\n",
    "\\mu_{BL} = \\left[ (\\tau \\Sigma)^{-1} + P^\\top \\Omega^{-1} P \\right]^{-1} \\left[ (\\tau \\Sigma)^{-1} \\Pi + P^\\top \\Omega^{-1} Q \\right],\n",
    "$$\n",
    "where $\\mu_{BL}$ is the expected return of the portfolio, $\\tau$ is the uncertainty in the prior, $\\Sigma$ is the covariance matrix of asset returns, $P$ is a picking matrix, with row vectors that represent the weights used in assets for expressing relative / absolute views, $\\Omega$ is the diagonal covariance matrix of confidences of our expected returns $Q$ (view specified by investor), and $\\Pi$ is the implied excess return of our assets.\n",
    "\n",
    "Now,\n",
    "$$\n",
    "\\Pi = \\lambda \\Sigma w_{mkt},\n",
    "$$\n",
    "where $\\lambda$ is the investor risk aversion co-efficient, and $w_{mkt}$ are the market capitalization weights of our assets, determined by TVL in our case (*_todo: expand on this_*).\n",
    "\n",
    "Before we jump into modelling, let's first explore our data. We extract the largest 40 assets by TVL."
   ],
   "id": "20fe158d64322d32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T21:53:33.482716Z",
     "start_time": "2025-05-25T21:53:27.832702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Exploring the universe of data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from main_app.infrastructure.defi_llama import get_pool_summary_data\n",
    "\n",
    "pool_map = get_pool_summary_data()\n",
    "df = pd.DataFrame()\n",
    "for symbol, pools in pool_map.items():\n",
    "    tvl = 0\n",
    "    weighted_apy = 0\n",
    "    num_pools = len(pools)\n",
    "    largest_pool_id = \"\"\n",
    "    largest_pool_tvl = 0\n",
    "    for pool in pools:\n",
    "        tvl += pool.tvlUsd\n",
    "        weighted_apy += pool.tvlUsd * pool.apy / 100\n",
    "        if pool.tvlUsd > largest_pool_tvl:\n",
    "            largest_pool_tvl = pool.tvlUsd\n",
    "            largest_pool_id = pool.pool\n",
    "\n",
    "    weighted_apy = weighted_apy / tvl\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'symbol': [symbol], 'tvlUsd': [tvl], 'apy': [weighted_apy], 'pools': [num_pools], 'largest_pool_id': [largest_pool_id], 'largest_pool_tvl': [largest_pool_tvl], 'largest_pool_pct_of_tvl': [largest_pool_tvl / tvl * 100]})])\n",
    "\n",
    "df.sort_values(by='tvlUsd', ascending=False)\n",
    "largest_assets_df = df.head(40)\n",
    "\n",
    "print(largest_assets_df.head(5))"
   ],
   "id": "81a9587c6999485b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol       tvlUsd       apy  pools                       largest_pool_id  \\\n",
      "0  STETH  22834298418  0.026812     18  747c1d2a-c668-4682-b9f9-296708a3dd90   \n",
      "0  WEETH  11436105113  0.018039     55  46bd2bdf-6d92-4066-b482-e885ee172264   \n",
      "0  WBETH   5898393331  0.025862      7  80b8bf92-b953-4c20-98ea-c9653ef2bb98   \n",
      "0   WBTC   6977942217  0.000604    132  7e382157-b1bc-406d-b17b-facba43b716e   \n",
      "0   RETH   3558687185  0.025322     30  d4b3c522-6127-4b89-bedf-83641cdcd2eb   \n",
      "\n",
      "   largest_pool_tvl  largest_pool_pct_of_tvl  \n",
      "0       22694796255                99.389067  \n",
      "0        6111246697                53.438182  \n",
      "0        5361948613                90.905240  \n",
      "0        4136468186                59.279198  \n",
      "0        3355782692                94.298333  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Importantly, note the _largest_pool_pct_of_tvl_, being the percentage of the total TVL (across all pools) that consists of the largest pool. We note that the largest pool is always significant enough to only look at the data from the largest pool as representative.",
   "id": "9f2757c84a56840f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Parameter estimation\n",
    "\n",
    "## Determining the risk aversion co-efficient\n",
    "We wish to determine the risk aversion coefficient, $\\lambda$, that represents the amount of risk an investor is willing to take. Higher $\\lambda$ means less risky, and conversely, lower $\\lambda$ means higher risk tolerance.\n",
    "\n",
    "In the Black-Litterman model used for Equities this is often set to 2.5, based on long-run estimates of equity risk premiums and volatility. Works as a good default for institutional settings.\n",
    "\n",
    "We wish to adapt this for the defi market by considering a DeFi benchmark portfolio, to be used as an index $I$, using the same method by using:\n",
    "$$\n",
    "\\lambda = \\frac{\\mathbb{E}[R_I]-r_f}{\\sigma_P^2},\n",
    "$$\n",
    "where $R_I$ are the index (portfolio) returns, $r_f$ is the risk free rate, and $\\sigma$ is the standard deviation of the index returns.\n",
    "\n",
    "First we load the data for the largest assets that will constitute the index."
   ],
   "id": "fa872522be6ef0c7"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from main_app.infrastructure.market_data import get_historical_data_for_symbol, load_symbol_to_address_mapping\n",
    "\n",
    "symbols = largest_assets_df['symbol'].to_list()\n",
    "largest_pools = largest_assets_df['largest_pool_id'].to_list()\n",
    "\n",
    "df_tvl = pd.DataFrame()\n",
    "df_apy = pd.DataFrame()\n",
    "df_price = pd.DataFrame()\n",
    "\n",
    "# Get current path and join with static data path\n",
    "current_path = os.getcwd()\n",
    "mapping_file_path = os.path.join(current_path, \"..\\static_data\\symbol_to_contract_address_map.json\")\n",
    "symbol_to_address_mapping = load_symbol_to_address_mapping(mapping_file_path)\n",
    "\n",
    "for symbol, pool in zip(symbols, largest_pools):\n",
    "    historic_data_df = get_historical_data_for_symbol(symbol, symbol_to_address_mapping, [pool])\n",
    "\n",
    "    # Convert the 'date' column to datetime\n",
    "    historic_data_df['date'] = pd.to_datetime(historic_data_df['date'])\n",
    "\n",
    "    # Select only the last entry for each day\n",
    "    historic_data_df = historic_data_df.sort_values(by='date').groupby(historic_data_df['date'].dt.date).last()\n",
    "\n",
    "    # Create TVL and APY series indexed by date\n",
    "    df_tvl[symbol] = historic_data_df['tvlUsd']\n",
    "    df_apy[symbol] = historic_data_df['apy'] / 100\n",
    "    df_price[symbol] = historic_data_df['price'] / 100\n",
    "\n",
    "# Only keep rows where there are no NaN values across all symbols\n",
    "df_tvl.dropna(inplace=True)\n",
    "df_apy.dropna(inplace=True)\n",
    "df_price.dropna(inplace=True)\n",
    "\n",
    "# Make sure that TVL, APY, and price dataframes align on the same set of dates\n",
    "common_days = df_tvl.index.intersection(df_apy.index).intersection(df_price.index)\n",
    "df_tvl = df_tvl.loc[common_days]\n",
    "df_apy = df_apy.loc[common_days]\n",
    "df_price = df_price.loc[common_days]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:02:53.194622Z",
     "start_time": "2025-05-25T22:02:39.819656Z"
    }
   },
   "id": "bda92795f102deda",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17184\\1217865196.py:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  mapping_file_path = os.path.join(current_path, \"..\\static_data\\symbol_to_contract_address_map.json\")\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17184\\1217865196.py:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  mapping_file_path = os.path.join(current_path, \"..\\static_data\\symbol_to_contract_address_map.json\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not dict",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     16\u001B[39m symbol_to_address_mapping = load_symbol_to_address_mapping(mapping_file_path)\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m symbol, pool \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(symbols, largest_pools):\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m     historic_data_df = \u001B[43mget_historical_data_for_symbol\u001B[49m\u001B[43m(\u001B[49m\u001B[43msymbol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msymbol_to_address_mapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mpool\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m     \u001B[38;5;66;03m# Convert the 'date' column to datetime\u001B[39;00m\n\u001B[32m     22\u001B[39m     historic_data_df[\u001B[33m'\u001B[39m\u001B[33mdate\u001B[39m\u001B[33m'\u001B[39m] = pd.to_datetime(historic_data_df[\u001B[33m'\u001B[39m\u001B[33mdate\u001B[39m\u001B[33m'\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\dev\\vv-chain-services\\src\\ml-engine\\main_app\\infrastructure\\market_data.py:24\u001B[39m, in \u001B[36mget_historical_data_for_symbol\u001B[39m\u001B[34m(symbol, mapping_file_path, pools)\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     18\u001B[39m \u001B[33;03mGet the combined historical data for a given symbol, including price, TVL, and APY.\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[33;03m:param symbol: The symbol of the coin (e.g., \"ETH\").\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[33;03m:param mapping_file_path: Path to the JSON file mapping symbols to contract addresses.\u001B[39;00m\n\u001B[32m     21\u001B[39m \u001B[33;03m:return: A pandas DataFrame containing price, TVL, and APY data.\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# Load the mapping and get the contract address for the symbol\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m symbol_to_address = \u001B[43mload_symbol_to_address_mapping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmapping_file_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m symbol \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m symbol_to_address:\n\u001B[32m     27\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSymbol \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msymbol\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not found in mapping.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\dev\\vv-chain-services\\src\\ml-engine\\main_app\\infrastructure\\market_data.py:12\u001B[39m, in \u001B[36mload_symbol_to_address_mapping\u001B[39m\u001B[34m(file_path)\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload_symbol_to_address_mapping\u001B[39m(file_path: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m      7\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[33;03m    Load the mapping of symbols to contract addresses from a JSON file.\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[33;03m    :param file_path: Path to the JSON file.\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[33;03m    :return: A dictionary mapping symbols to contract addresses.\u001B[39;00m\n\u001B[32m     11\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mr\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m     13\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m json.load(f)\n",
      "\u001B[31mTypeError\u001B[39m: expected str, bytes or os.PathLike object, not dict"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next we construct the index and perform the calcs to get $\\lambda$",
   "id": "3a068d8f31c54a8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Construct index and perform calcs\n",
    "\n",
    "# Calculate daily log returns for df_price\n",
    "df_price_log_returns = np.log(df_price / df_price.shift(1)).dropna().to_numpy()\n",
    "\n",
    "# Calculate weight_market and target_return\n",
    "mean_market_weight_vector = df_tvl.mean().to_numpy()\n",
    "\n",
    "# create index and calculate prices and returns\n",
    "index_0 = 100\n",
    "index_weights = mean_market_weight_vector / sum(mean_market_weight_vector)\n",
    "index_daily_returns = (index_weights.transpose() @ df_price_log_returns.transpose()) * (1 + df_apy.to_numpy().transpose() / 365.0)\n",
    "index_prices = index_0 * np.cumprod(np.exp(index_daily_returns))\n",
    "\n",
    "# calculate index mean return and std dev\n",
    "index_mean_return = np.mean(index_prices)\n",
    "index_std_dev = np.std(index_prices)\n",
    "\n",
    "# specify risk-free rate\n",
    "risk_free_rate = 0.02 / 365\n",
    "\n",
    "# calculate lambda\n",
    "l = (index_mean_return - risk_free_rate) / index_std_dev ** 2\n",
    "\n",
    "# Calculate covariance matrix of target_return with itself\n",
    "# cov_target_return = np.cov(target_return_matrix, rowvar=False, ddof=0)\n",
    "\n",
    "# Calculate lambda\n",
    "#numerator = mean_market_weight_vector.dot(mean_target_return_vector)\n",
    "#denominator = mean_market_weight_vector.transpose() @ cov_target_return @ mean_market_weight_vector\n",
    "#l = numerator / denominator\n",
    "\n",
    "print(f\"Lambda = {l}\")"
   ],
   "id": "ea337c3a4dac02f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e04639143953948"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
